lab,product,claim_text,claim_type,claim_theme,published_date,source_url,supporting_details,notes
Anthropic,Claude Opus 4,"Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%).",Benchmark result,Coding performance,2025-05-22,https://www.anthropic.com/news/claude-4,Headline section enumerates benchmark percentages for SWE-bench and Terminal-bench.,Publication date aligned with Claude 4 launch window; confirm via internal release log.
Anthropic,Claude Sonnet 4,"Claude Sonnet 4 significantly improves on Sonnet 3.7's industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench.",Benchmark result,Coding performance,2025-05-22,https://www.anthropic.com/news/claude-4,Same launch article quantifies Sonnet 4's SWE-bench score.,Verify launch date before export.
Anthropic,Claude 4 family,Both models are 65% less likely to engage in this behavior than Sonnet 3.7 on agentic tasks that are particularly susceptible to shortcuts and loopholes.,Behavioral improvement,Safety and reliability,2025-05-22,https://www.anthropic.com/news/claude-4,Article quantifies reduction in shortcut-seeking behavior relative to Sonnet 3.7.,Behavior measurement covers agentic task suite; details in appendix.
Anthropic,Claude Sonnet 4.5,"Claude Sonnet 4.5 represents a significant leap forward on computer use. On OSWorld, a benchmark that tests AI models on real-world computer tasks, Sonnet 4.5 now leads at 61.4%. Just four months ago, Sonnet 4 held the lead at 42.2%.",Benchmark result,Computer use,2025-09-29,https://www.anthropic.com/news/claude-sonnet-4-5,Marketing post compares OSWorld Verified scores across model generations.,Dates derived from Claude Sonnet product timeline; confirm annually.
Anthropic,Claude Sonnet 4.5,"Practically speaking, we have observed it maintaining focus for more than 30 hours on complex, multi-step tasks.",Usage capability,Agent endurance,2025-09-29,https://www.anthropic.com/news/claude-sonnet-4-5,Statement highlights extended task persistence in launch narrative.,Qualifies qualitative endurance claim with quantitative runtime.
Anthropic,Claude Sonnet 4.5,"SWE-bench Verified: All Claude results were reported using a simple scaffold with two tools - bash and file editing via string replacements. We report 77.2%, which was averaged over 10 trials, no test-time compute, and 200K thinking budget on the full 500-problem SWE-bench Verified dataset.",Benchmark result,Coding performance,2025-09-29,https://www.anthropic.com/news/claude-sonnet-4-5,Footnote specifies methodology and aggregate score for Sonnet 4.5 on SWE-bench Verified.,Capture methodological context for claim reconciliation.
Anthropic,Claude Opus 4.1,"Pricing for Claude Opus 4.1 starts at $15 per million input tokens and $75 per million output tokens, with up to 90% cost savings with prompt caching and 50% cost savings with batch processing.",Pricing,Cost structure,2025-08-05,https://www.anthropic.com/claude/opus,Product page outlines price points and advertised savings for caching and batching.,Monitor for pricing updates across API tiers.
Anthropic,Claude Opus 4.1,We also saw impressive efficiency gains: up to 50% faster task completion with 45% fewer tool uses.,User efficiency,Productivity,2025-08-05,https://www.anthropic.com/claude/opus,Customer testimonial section quantifies observed efficiency improvements.,Derived from partner quote on Opus 4.1 page.
OpenAI,GPT-5,"It sets a new state of the art across math (94.6% on AIME 2025 without tools), real-world coding (74.9% on SWE-bench Verified, 88% on Aider Polyglot), multimodal understanding (84.2% on MMMU), and health (46.2% on HealthBench Hard).",Benchmark result,Multi-domain performance,2025-10-01,https://openai.com/blog/introducing-gpt-5,Launch blog enumerates headline benchmark scores for GPT-5 across key domains.,Percentages drawn directly from public launch copy; validate against system card.
OpenAI,GPT-5,"With GPT-5 pro's extended reasoning, the model also sets a new SOTA on GPQA, scoring 88.4% without tools.",Benchmark result,Graduate reasoning,2025-10-01,https://openai.com/blog/introducing-gpt-5,Same article highlights GPQA improvement for GPT-5 pro mode.,Aligns with GPQA diamond reporting in system card.
OpenAI,GPT-5,"In our evaluations, GPT-5 (with thinking) performs better than OpenAI o3 with 50-80% less output tokens across capabilities, including visual reasoning, agentic coding, and graduate-level scientific problem solving.",Efficiency claim,Reasoning efficiency,2025-10-01,https://openai.com/blog/introducing-gpt-5,Launch blog compares token usage efficiency versus OpenAI o3.,Quantified range reflects evaluation span; no single benchmark specified.
OpenAI,GPT-5,"With web search enabled on anonymized prompts representative of ChatGPT production traffic, GPT-5's responses are ~45% less likely to contain a factual error than GPT-4o, and when thinking, GPT-5's responses are ~80% less likely to contain a factual error than OpenAI o3.",Quality claim,Factual accuracy,2025-10-01,https://openai.com/blog/introducing-gpt-5,Blog quantifies hallucination reductions versus GPT-4o and o3 baselines.,Replicate evaluation setup before using claim externally.
OpenAI,GPT-5,"We removed all the images from the prompts of the multimodal benchmark CharXiv, and found that OpenAI o3 still gave confident answers about non-existent images 86.7% of the time, compared to just 9% for GPT-5.",Reliability claim,Honesty under uncertainty,2025-10-01,https://openai.com/blog/introducing-gpt-5,Ablation test details how GPT-5 reduces hallucinated answers on CharXiv.,Ensure benchmark variant matches CharXiv removal experiment.
OpenAI,GPT-5,"On a large set of conversations representative of real production ChatGPT traffic, we have reduced rates of deception from 4.8% for o3 to 2.1% of GPT-5 reasoning responses.",Behavioral improvement,Deception mitigation,2025-10-01,https://openai.com/blog/introducing-gpt-5,Launch blog quantifies deception reduction in production-like conversations.,Rates depend on internal deception classifier thresholds.
OpenAI,GPT-5,"In targeted sycophancy evaluations using prompts specifically designed to elicit sycophantic responses, GPT-5 meaningfully reduced sycophantic replies (from 14.5% to less than 6%).",Behavioral improvement,Sycophancy reduction,2025-10-01,https://openai.com/blog/introducing-gpt-5,Blog references targeted sycophancy eval set with quantitative drop.,Capture evaluation protocol for auditing.
OpenAI,GPT-5 pro,"External experts preferred GPT-5 pro over 'GPT-5 thinking' 67.8% of the time. GPT-5 pro made 22% fewer major errors and excelled in health, science, mathematics, and coding.",User study,Human preference,2025-10-01,https://openai.com/blog/introducing-gpt-5,Launch blog summarizes expert evaluation preference rates and error reduction.,Clarify sample size (1000 prompts) when presenting externally.
OpenAI,GPT-5 API,"GPT-5 is state-of-the-art across key coding benchmarks, scoring 74.9% on SWE-bench Verified and 88% on Aider polyglot. The model also excels at front-end coding, beating OpenAI o3 at frontend web development 70% of the time in internal testing.",Benchmark result,Developer performance,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog headline paragraph quantifies multiple coding wins.,Internal testing metric (70%) should note evaluation harness when reused.
OpenAI,GPT-5 API,GPT-5 also excels at long-running agentic tasks - achieving SOTA results on tau 2-bench telecom (96.7%).,Benchmark result,Tool use,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog spotlights tau 2-bench telecom score.,Benchmark origin: Sierra tau 2-bench (2025).
OpenAI,GPT-5 API,"On SWE-bench Verified, GPT-5 scores 74.9%, up from o3's 69.1%. Notably, GPT-5 uses 22% fewer output tokens and 45% fewer tool calls.",Benchmark delta,Efficiency,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog contrasts GPT-5 vs o3 on SWE-bench resource usage.,Token/tool reductions measured at high reasoning effort.
OpenAI,GPT-5 API,"On Aider polyglot, an evaluation of code editing, GPT-5 sets a new record of 88%, a one-third reduction in error rate compared to o3.",Benchmark result,Code editing,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog calls out error-rate reduction on Aider polyglot diff set.,Relative error-rate reduction should be cross-validated with Aider maintainers.
OpenAI,GPT-5 API,"In side-by-side comparisons with o3, GPT-5 was preferred by our testers 70% of the time.",User study,Front-end generation,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog details internal tester preference rate for front-end tasks.,Preference study context: front-end web development scenarios.
OpenAI,GPT-5 API,GPT-5 sets new records on benchmarks of instruction following (69.6% on Scale MultiChallenge) and tool calling (96.7% on tau 2-bench telecom).,Benchmark result,Instruction following,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog lists paired benchmark wins for instruction following and tool use.,Scale MultiChallenge graded with o3-mini evaluator.
OpenAI,GPT-5 API,"In their publication, no model scored above 49%. GPT-5 scores 97%.",Benchmark result,Tool use,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog references Sierra tau 2-bench telecom paper comparison.,Explicitly refers to tau 2-bench telecom study (June 2025).
OpenAI,GPT-5 API,"On inputs that are 128K-256K tokens, GPT-5 gives the correct answer 89% of the time.",Benchmark result,Long context,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,BrowseComp Long Context benchmark description includes 89% accuracy figure.,Dataset available on Hugging Face; replicate to validate.
OpenAI,GPT-5 API,"On prompts from LongFact and FactScore benchmarks, GPT-5 makes about 80% fewer factual errors than o3.",Quality claim,Hallucination reduction,2025-10-01,https://openai.com/blog/introducing-gpt-5-for-developers/,Developer blog compares hallucination rates across factuality benchmarks.,Quantify baseline error rates for precise reduction calculation.
Meta,Llama 3,"Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages.",Training data,Data scale,2024-04-18,https://ai.meta.com/blog/meta-llama-3/,"Launch blog quantifies dataset size, growth, and language composition.",Values derived from pretraining section; ensure data pipeline documentation agrees.
Meta,Llama 3,"Those improvements resulted in an overall effective training time of more than 95%. Combined, these improvements increased the efficiency of Llama 3 training by about three times compared to Llama 2.",Training efficiency,Infrastructure,2024-04-18,https://ai.meta.com/blog/meta-llama-3/,Training infrastructure section summarizes GPU utilization gains.,'About three times' quoted as '~three times' in source.
Meta,Llama 3,"Our benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2.",Efficiency claim,Tokenization,2024-04-18,https://ai.meta.com/blog/meta-llama-3/,Tokenizer section quantifies efficiency increase over Llama 2.,Applies to 8B variant with new tokenizer.
Mistral AI,Pixtral 12B,"Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models.",Benchmark result,Multimodal reasoning,2024-09-18,https://mistral.ai/news/pixtral-12b/,Launch blog headline calls out MMMU score versus larger competitors.,MMMU value pulled from evaluation harness summary.
Mistral AI,Pixtral 12B,"It substantially outperforms Qwen2-VL 7B, LLaVa-OneVision 7B and Phi-3.5 Vision in instruction following, with a 20% relative improvement in text IF-Eval and MT-Bench over the nearest OSS model.",Benchmark result,Instruction following,2024-09-18,https://mistral.ai/news/pixtral-12b/,Performance section quantifies relative gains on text and multimodal instruction benchmarks.,Nearest OSS baseline unspecified; cross-check technical report.
xAI,Grok-2,"We evaluated the Grok-2 models across a series of academic benchmarks... Grok-2 mini demonstrates significant improvements over our previous Grok-1.5 model, lifting MMLU-Pro from 51.0% to 75.5%.",Benchmark delta,General knowledge,2025-09-10,https://x.ai/blog/grok-2,Benchmark table lists Grok-1.5 and Grok-2 MMLU-Pro scores side by side.,Extracted from published comparison table; verify table version for updates.
xAI,Grok-2 mini,Benchmark rows show Grok-2 mini raising HumanEval pass@1 from 74.1% to 85.7% compared to Grok-1.5.,Benchmark delta,Coding performance,2025-09-10,https://x.ai/blog/grok-2,HumanEval row in the benchmark table reports pass@1 values for Grok variants.,Table values represent zero-shot pass rates; confirm evaluation protocol.
xAI,Grok-2,"Benchmark table reports Grok-2 achieving 69.0% on MathVista, ahead of GPT-4 Turbo at 58.1% and Claude 3 Opus at 50.5%.",Benchmark comparison,Visual math reasoning,2025-09-10,https://x.ai/blog/grok-2,MathVista row in the published table contrasts Grok-2 against closed models.,Doc lists MathVista score as 69.0% under Grok-2 column.
