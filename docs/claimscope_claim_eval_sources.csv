Title,Source,Summary,Access & license notes,Relevance score,Follow-up actions
"OpenAI Evals Registry","https://github.com/openai/evals","Open-source registry of YAML/JSON benchmarks spanning QA, reasoning, safety, and adversarial prompts, each with scoring configs and model-graded templates. Tasks bundle prompts, expected outputs, and evaluation metrics so claims about model behavior can be tied to structured evidence.","Clone the repo and run `git lfs fetch --all` to pull datasets; README states contributed eval logic/data are shared under MIT; running many evals incurs OpenAI API costs.","High","Sync Git LFS assets, catalog tasks that expose claim/evidence fields, and wire them into Claimscope’s ingestion pipeline."
"OpenAI Moderation Evaluation Dataset (Holistic Undesired Content)","https://github.com/openai/moderation-api-release","Test set for “A Holistic Approach to Undesired Content Detection” with 1,680 JSONL samples labeled across harassment, hate, sexual, violence, and self-harm categories. Each entry supplies the raw snippet plus taxonomy labels suited to calibrating claim classification and safety thresholds.","Download `data/samples-1680.jsonl.gz`; contents are under the MIT License per repository metadata.","Medium","Fetch and decompress the JSONL file, normalize category labels to Claimscope’s schema, and benchmark moderation claim detectors."
"GPT-4 Technical Report Evaluation Tables","https://arxiv.org/abs/2303.08774","100-page system card enumerating >50 benchmarks with task names, methodologies, seeds, and metrics comparing GPT-4 variants to baselines. Tables and narrative claims include direct evidence links (figures, appendices) valuable for grounding performance statements.","Freely accessible via arXiv; distributed under arXiv’s perpetual, non-exclusive license; PDF and HTML available.","Medium","Programmatically parse the report’s tables to extract structured claims (task, metric, model version) and link to cited evidence snippets."
"Anthropic Model-Written Evaluation Datasets","https://github.com/anthropics/evals","Collections of persona, sycophancy, advanced AI risk, and winogender evals generated by language models with human validation metrics. Prompts, expected answers, and metadata support tracing risky-behavior claims back to textual evidence.","Clone repo; datasets stored in subdirectories; licensed under CC-BY 4.0 according to repository settings.","High","Pull targeted subsets (e.g., `advanced-ai-risk/`), convert prompts/labels into Claimscope claim objects, and compare human vs model annotations."
"Anthropic Rogue Deploy Eval Harness","https://github.com/anthropics/rogue-deploy-eval","Toy evaluation measuring whether a model can modify deployment monitors while solving auxiliary tasks, mirroring Anthropic’s internal rogue deployment study. Ships prompt templates and scoring scripts to surface claims about code-tampering competence.","MIT-licensed; repository notes required placeholders (`TO_FILL`) for private inference/back-end tooling that teams must supply.","Medium","Implement a local inference loop or coordinate with Anthropic partners to fill execution hooks before harvesting evaluation traces."
"DeepMind Dangerous Capability Evaluations Resources","https://github.com/google-deepmind/dangerous-capability-evaluations","Reproduction kit for the “Evaluating Frontier Models for Dangerous Capabilities” study, including JSON challenge specs, Dockerfiles, and vulnerable app code for CTF, self-proliferation, and self-reasoning tasks. Each scenario encodes explicit success criteria enabling grounded claims about hazardous capabilities.","Software under Apache-2.0 and supporting materials under CC-BY 4.0; flags and some solutions removed—users must randomize placeholders and may need to contact maintainers for withheld artifacts.","High","Randomize challenge flags, provision sandbox infrastructure, and request any missing solution files to run end-to-end dangerous capability audits."
"DeepMind Counterfactual Fairness Evaluation Dataset","https://github.com/google-deepmind/counterfactual_fairness_evaluation_dataset","Dataset of 2,402 seed prompts plus 15,874 LLM-generated counterfactual variants balanced across religion, gender identity, race/ethnicity, and sexuality. Provides ground-truth hate, sexual, toxicity, and violence labels to substantiate fairness and safety claims.","CSV downloads (`counterfactual_fairness_dataset_seeds.csv`, `..._expanded.csv`) under Apache-2.0; documentation details LLM rewrite pipeline and manual filtering heuristics.","High","Download both CSVs, preserve `example_key` groupings, and integrate counterfactual pairs into fairness regression tests."
"Meta Purple Llama CyberSecEval 4 Benchmark Suite","https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/README.md","CyberSecEval 4 extends prior Purple Llama benchmarks with MITRE attack compliance, false refusal, secure coding, prompt injection (text/vision), code interpreter abuse, spear phishing, autonomous offensive ops, and AutoPatch tests. Detailed runbooks describe prompt flows, judge models, and metrics for transparent safety claims.","Benchmark components distributed under permissive licenses (MIT noted for evals/benchmarks); requires API keys for judge/expansion models and optional CrowdStrike submodule (`CyberSOCEval_data`).","High","Install dependencies, initialize submodules, configure judge APIs, and pipeline resulting traces into Claimscope’s claim/evidence store."
"Mistral MM-MT-Bench Multimodal Dialogue Eval","https://huggingface.co/datasets/mistralai/MM-MT-Bench","92 multi-turn multimodal conversations with charts, documents, and open-ended questions paired with reference answers and GPT-4o-as-judge scores. Captures granular judgment metadata to ground claims about instruction-following quality across visual modalities.","Downloadable via Hugging Face Hub (Parquet format, ~36 MB compressed); dataset card omits an explicit license—confirm permitted use with Mistral before redistribution.","High","Use `huggingface-cli` to pull the dataset, log licensing confirmation, and adapt judge annotations into Claimscope’s claim-to-evidence alignment."
